name: Hourly Data Pipeline (Every Hour - 6 Hours Collection)

on:
  schedule:
    # Run every hour
    - cron: '0 * * * *'
  workflow_dispatch: # Allow manual trigger

jobs:
  data-pipeline:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements-ci.txt
        echo "âœ… Dependencies installed successfully"
        
    - name: Create data directories
      run: |
        mkdir -p data_repositories/hourly_data/raw
        mkdir -p data_repositories/hourly_data/processed
        mkdir -p data_repositories/historical_data/raw
        mkdir -p data_repositories/historical_data/processed
        mkdir -p data_repositories/features
        
    - name: Set up environment variables
      run: |
        echo "OPENWEATHER_API_KEY=${{ secrets.OPENWEATHER_API_KEY }}" >> $GITHUB_ENV
        echo "METEOSTAT_API_KEY=${{ secrets.METEOSTAT_API_KEY }}" >> $GITHUB_ENV
        
    - name: Load and verify historical data
      run: |
        echo "ğŸ“‚ Loading historical data from repository..."
        if [ -f "data_repositories/historical_data/processed/merged_data.csv" ]; then
          echo "âœ… Historical data found: data_repositories/historical_data/processed/merged_data.csv"
          echo "ğŸ“Š Historical data info:"
          python -c "import pandas as pd; df = pd.read_csv('data_repositories/historical_data/processed/merged_data.csv'); print(f'Records: {len(df):,}'); print(f'Columns: {len(df.columns)}'); print(f'Date range: {df[\"timestamp\"].min()} to {df[\"timestamp\"].max()}' if 'timestamp' in df.columns else 'No timestamp column')"
        else
          echo "âš ï¸ No historical data found. Will create new dataset."
        fi
        
    - name: Collect new data (smart collection)
      run: |
        echo "ğŸ”„ Starting smart data collection..."
        echo "ğŸ“… Current time: $(date -u)"
        echo "â° Automatically detecting last timestamp and collecting new data..."
        
        # Run the smart data collection script
        python collect_6hours.py
        
        echo "âœ… Data collection completed"
    
    - name: Finalize merged dataset (resolve pending file)
      run: |
        set -euo pipefail
        MASTER="data_repositories/historical_data/processed/merged_data.csv"
        # Find newest pending file (if any)
        LATEST_PENDING=$(find data_repositories/historical_data/processed -maxdepth 1 -type f -name 'merged_data.csv.pending_*' -printf '%T@ %p\n' | sort -nr | awk 'NR==1{print $2}')
        if [ ! -f "$MASTER" ] && [ -n "${LATEST_PENDING:-}" ]; then
          echo "âš ï¸ Merged dataset missing; promoting pending file: $LATEST_PENDING"
          mv "$LATEST_PENDING" "$MASTER"
        else
          echo "â„¹ï¸ No promotion needed (master present or no pending file)"
        fi
        
    - name: Verify merged dataset
      run: |
        set -euo pipefail
        echo "ğŸ” Verifying merged dataset after collection..."
        MASTER="data_repositories/historical_data/processed/merged_data.csv"
        if [ -f "$MASTER" ]; then
          echo "âœ… Merged dataset updated successfully"
          echo "ğŸ“Š Updated dataset info:"
          python -c "import pandas as pd; df=pd.read_csv('data_repositories/historical_data/processed/merged_data.csv'); print('   Total records:', len(df)); has_ts=('timestamp' in df.columns); ts=pd.to_datetime(df['timestamp']) if has_ts else None; print('   Date range:', ts.min(), 'to', ts.max()) if has_ts else None; print('   Latest timestamp:', ts.max()) if has_ts else None; print('   Total columns:', len(df.columns))"
        else
          echo "âŒ Merged dataset not found after collection â€“ checking for pending file..."
          LATEST_PENDING=$(find data_repositories/historical_data/processed -maxdepth 1 -type f -name 'merged_data.csv.pending_*' -printf '%T@ %p\n' | sort -nr | awk 'NR==1{print $2}')
          if [ -n "${LATEST_PENDING:-}" ]; then
            echo "ğŸ”„ Found pending file: $LATEST_PENDING"
            mv "$LATEST_PENDING" "$MASTER"
            echo "âœ… Promoted pending file to merged_data.csv"
            python -c "import pandas as pd; df=pd.read_csv('data_repositories/historical_data/processed/merged_data.csv'); print('   Total records:', len(df)); has_ts=('timestamp' in df.columns); ts=pd.to_datetime(df['timestamp']) if has_ts else None; print('   Date range:', ts.min(), 'to', ts.max()) if has_ts else None; print('   Latest timestamp:', ts.max()) if has_ts else None; print('   Total columns:', len(df.columns))"
          else
            echo "âŒ No pending file found either; failing the pipeline"
            exit 1
          fi
        fi
        
    - name: Run data preprocessing (script 02)
      run: |
        echo "ğŸ”„ Starting data preprocessing (script 02)..."
        echo "ğŸ“ Input: data_repositories/historical_data/processed/merged_data.csv"
        python phase2_data_preprocessing.py
        
        # Verify output
        if [ -f "data_repositories/features/phase1_no_leakage_data.csv" ]; then
          echo "âœ… Preprocessing completed successfully"
          echo "ğŸ“Š Preprocessing output info:"
          python -c "import pandas as pd; df = pd.read_csv('data_repositories/features/phase1_no_leakage_data.csv'); print(f'Records: {len(df):,}'); print(f'Features: {len(df.columns)}'); print(f'Date range: {df[\"timestamp\"].min()} to {df[\"timestamp\"].max()}' if 'timestamp' in df.columns else 'No timestamp column')"
        else
          echo "âŒ Preprocessing output not found"
          exit 1
        fi
        
    - name: Run feature selection (script 03)
      run: |
        echo "ğŸ”„ Starting feature selection (script 03)..."
        echo "ğŸ“ Input: data_repositories/features/phase1_no_leakage_data.csv"
        python phase3_feature_selection.py
        
        # Verify output
        if [ -f "data_repositories/features/phase1_fixed_selected_features.csv" ]; then
          echo "âœ… Feature selection completed successfully"
          echo "ğŸ“Š Feature selection output info:"
          python -c "import pandas as pd; df = pd.read_csv('data_repositories/features/phase1_fixed_selected_features.csv'); print(f'Records: {len(df):,}'); print(f'Selected features: {len(df.columns)}'); print(f'Date range: {df[\"timestamp\"].min()} to {df[\"timestamp\"].max()}' if 'timestamp' in df.columns else 'No timestamp column')"
        else
          echo "âŒ Feature selection output not found"
          exit 1
        fi
        
    - name: Commit and push all updated data
      run: |
        echo "ğŸ’¾ Committing all updated datasets to GitHub..."
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        
        # Add all data files
        git add data_repositories/
        
        # Check if there are changes to commit
        if git diff --staged --quiet; then
          echo "â„¹ï¸ No changes to commit"
        else
          # Get current timestamp for commit message
          TIMESTAMP=$(date -u +'%Y-%m-%d %H:%M:%S UTC')
          
          # Create detailed commit message
          git commit -m "ğŸ¤– Auto-update: Hourly data pipeline - $TIMESTAMP - 6 hours data collected, scripts 02 and 03 executed, all datasets committed"
          
          # Push to GitHub
          git push
          echo "âœ… All datasets committed and pushed to GitHub successfully"
        fi
        
    - name: Upload data artifacts
      uses: actions/upload-artifact@v4
      with:
        name: hourly-data-pipeline-artifacts
        path: |
          data_repositories/
        retention-days: 7
        
    - name: Pipeline summary
      run: |
        echo ""
        echo "ğŸ‰ HOURLY DATA PIPELINE COMPLETED SUCCESSFULLY!"
        echo "=================================================================="
        echo "ğŸ“Š Pipeline executed at: $(date -u)"
        echo "ğŸ”„ Data collection: Smart collection from last timestamp"
        echo "ğŸ“ Data merged into: data_repositories/historical_data/processed/merged_data.csv"
        echo "ğŸ”§ Script 02 executed: Data preprocessing completed"
        echo "ğŸ¯ Script 03 executed: Feature selection completed"
        echo "ğŸ’¾ All datasets committed to GitHub repository"
        echo "ğŸ“‚ Artifacts uploaded for next pipeline steps"
        echo "ğŸ”„ Next run scheduled for: $(date -u -d '+1 hour')"
        echo "=================================================================="
        
        # Final verification
        echo ""
        echo "ğŸ” FINAL VERIFICATION:"
        if [ -f "data_repositories/historical_data/processed/merged_data.csv" ]; then
          echo "âœ… Historical merged dataset: READY"
        else
          echo "âŒ Historical merged dataset: MISSING"
        fi
        
        if [ -f "data_repositories/features/phase1_no_leakage_data.csv" ]; then
          echo "âœ… Preprocessed features: READY"
        else
          echo "âŒ Preprocessed features: MISSING"
        fi
        
        if [ -f "data_repositories/features/phase1_fixed_selected_features.csv" ]; then
          echo "âœ… Selected features: READY"
        else
          echo "âŒ Selected features: MISSING"
        fi
        
        echo ""
        echo "ğŸš€ Pipeline ready for next steps (ML training, forecasting, etc.)"
