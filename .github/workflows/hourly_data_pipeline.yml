name: Hourly Data Pipeline (Every Hour - 6 Hours Collection)

on:
  schedule:
    # Run every hour
    - cron: '0 * * * *'
  workflow_dispatch: # Allow manual trigger

jobs:
  data-pipeline:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements-ci.txt
        echo "âœ… Dependencies installed successfully"
        
    - name: Create data directories
      run: |
        mkdir -p data_repositories/hourly_data/raw
        mkdir -p data_repositories/hourly_data/processed
        mkdir -p data_repositories/historical_data/raw
        mkdir -p data_repositories/historical_data/processed
        mkdir -p data_repositories/features
        
    - name: Set up environment variables
      run: |
        echo "OPENWEATHER_API_KEY=${{ secrets.OPENWEATHER_API_KEY }}" >> $GITHUB_ENV
        echo "METEOSTAT_API_KEY=${{ secrets.METEOSTAT_API_KEY }}" >> $GITHUB_ENV
        
    - name: Load and verify historical data
      run: |
        echo "ğŸ“‚ Loading historical data from repository..."
        if [ -f "data_repositories/historical_data/processed/merged_data.csv" ]; then
          echo "âœ… Historical data found: data_repositories/historical_data/processed/merged_data.csv"
          echo "ğŸ“Š Historical data info:"
          python -c "
          import pandas as pd
          try:
              df = pd.read_csv('data_repositories/historical_data/processed/merged_data.csv')
              print(f'   Records: {len(df):,}')
              if 'timestamp' in df.columns:
                  df['timestamp'] = pd.to_datetime(df['timestamp'])
                  print(f'   Date range: {df[\"timestamp\"].min()} to {df[\"timestamp\"].max()}')
              print(f'   Columns: {len(df.columns)}')
          except Exception as e:
              print(f'   Error reading historical data: {e}')
          "
        else
          echo "âš ï¸ No historical data found. Will create new dataset."
        fi
        
    - name: Create 6-hour collection script
      run: |
        echo "ğŸ“ Creating 6-hour data collection script..."
        echo 'import os' > collect_6hours.py
        echo 'import sys' >> collect_6hours.py
        echo 'sys.path.append(".")' >> collect_6hours.py
        echo 'from datetime import datetime, timedelta' >> collect_6hours.py
        echo '' >> collect_6hours.py
        echo '# Import the DataCollector class' >> collect_6hours.py
        echo 'from 01_data_collection import DataCollector' >> collect_6hours.py
        echo '' >> collect_6hours.py
        echo '# Calculate 6 hours ago from now' >> collect_6hours.py
        echo 'now = datetime.now()' >> collect_6hours.py
        echo 'start_time = now - timedelta(hours=6)' >> collect_6hours.py
        echo '' >> collect_6hours.py
        echo 'print(f"ğŸ• Collection window: {start_time} to {now}")' >> collect_6hours.py
        echo 'print(f"â° Duration: 6 hours")' >> collect_6hours.py
        echo '' >> collect_6hours.py
        echo '# Create collector with specific time window' >> collect_6hours.py
        echo 'collector = DataCollector(start_date=start_time, end_date=now, mode="hourly")' >> collect_6hours.py
        echo 'success = collector.run_pipeline()' >> collect_6hours.py
        echo '' >> collect_6hours.py
        echo 'if success:' >> collect_6hours.py
        echo '    print("âœ… 6-hour data collection completed successfully")' >> collect_6hours.py
        echo 'else:' >> collect_6hours.py
        echo '    print("âŒ 6-hour data collection failed")' >> collect_6hours.py
        echo '    sys.exit(1)' >> collect_6hours.py
        echo "âœ… Script created successfully"
        
    - name: Collect new data (past 6 hours)
      run: |
        echo "ğŸ”„ Starting data collection (past 6 hours)..."
        echo "ğŸ“… Current time: $(date -u)"
        echo "â° Collecting data for past 6 hours..."
        python collect_6hours.py
        echo "âœ… Data collection completed"
        
    - name: Verify merged dataset
      run: |
        echo "ğŸ” Verifying merged dataset after collection..."
        if [ -f "data_repositories/historical_data/processed/merged_data.csv" ]; then
          echo "âœ… Merged dataset updated successfully"
          echo "ğŸ“Š Updated dataset info:"
          python -c "
          import pandas as pd
          try:
              df = pd.read_csv('data_repositories/historical_data/processed/merged_data.csv')
              print(f'   Total records: {len(df):,}')
              if 'timestamp' in df.columns:
                  df['timestamp'] = pd.to_datetime(df['timestamp'])
                  print(f'   Date range: {df[\"timestamp\"].min()} to {df[\"timestamp\"].max()}')
                  print(f'   Latest timestamp: {df[\"timestamp\"].max()}')
              print(f'   Total columns: {len(df.columns)}')
          except Exception as e:
              print(f'   Error reading updated dataset: {e}')
          "
        else
          echo "âŒ Merged dataset not found after collection"
          exit 1
        fi
        
    - name: Run data preprocessing (script 02)
      run: |
        echo "ğŸ”„ Starting data preprocessing (script 02)..."
        echo "ğŸ“ Input: data_repositories/historical_data/processed/merged_data.csv"
        python 02_data_preprocessing.py
        
        # Verify output
        if [ -f "data_repositories/features/phase1_no_leakage_data.csv" ]; then
          echo "âœ… Preprocessing completed successfully"
          echo "ğŸ“Š Preprocessing output info:"
          python -c "
          import pandas as pd
          try:
              df = pd.read_csv('data_repositories/features/phase1_no_leakage_data.csv')
              print(f'   Records: {len(df):,}')
              if 'timestamp' in df.columns:
                  df['timestamp'] = pd.to_datetime(df['timestamp'])
                  print(f'   Date range: {df[\"timestamp\"].min()} to {df[\"timestamp\"].max()}')
              print(f'   Features: {len(df.columns)}')
          except Exception as e:
              print(f'   Error reading preprocessing output: {e}')
          "
        else
          echo "âŒ Preprocessing output not found"
          exit 1
        fi
        
    - name: Run feature selection (script 03)
      run: |
        echo "ğŸ”„ Starting feature selection (script 03)..."
        echo "ğŸ“ Input: data_repositories/features/phase1_no_leakage_data.csv"
        python 03_feature_selection.py
        
        # Verify output
        if [ -f "data_repositories/features/phase1_fixed_selected_features.csv" ]; then
          echo "âœ… Feature selection completed successfully"
          echo "ğŸ“Š Feature selection output info:"
          python -c "
          import pandas as pd
          try:
              df = pd.read_csv('data_repositories/features/phase1_fixed_selected_features.csv')
              print(f'   Records: {len(df):,}')
              if 'timestamp' in df.columns:
                  df['timestamp'] = pd.to_datetime(df['timestamp'])
                  print(f'   Date range: {df[\"timestamp\"].min()} to {df[\"timestamp\"].max()}')
              print(f'   Selected features: {len(df.columns)}')
          except Exception as e:
              print(f'   Error reading feature selection output: {e}')
          "
        else
          echo "âŒ Feature selection output not found"
          exit 1
        fi
        
    - name: Commit and push all updated data
      run: |
        echo "ğŸ’¾ Committing all updated datasets to GitHub..."
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        
        # Add all data files
        git add data_repositories/
        
        # Check if there are changes to commit
        if git diff --staged --quiet; then
          echo "â„¹ï¸ No changes to commit"
        else
          # Get current timestamp for commit message
          TIMESTAMP=$(date -u +'%Y-%m-%d %H:%M:%S UTC')
          
          # Create detailed commit message
          git commit -m "ğŸ¤– Auto-update: Hourly data pipeline - $TIMESTAMP

ğŸ”„ Data Collection: 6 hours of new weather and pollution data
ğŸ“Š Data Processing: Scripts 02 and 03 applied to merged dataset
ğŸ’¾ All datasets updated and committed:
  - Historical merged data (merged_data.csv)
  - Preprocessed features (phase1_no_leakage_data.csv)
  - Selected features (phase1_fixed_selected_features.csv)
  - Raw weather and pollution data
  - Metadata and backups

Pipeline: Collection â†’ Merge â†’ Preprocess â†’ Feature Selection â†’ Commit"
          
          # Push to GitHub
          git push
          echo "âœ… All datasets committed and pushed to GitHub successfully"
        fi
        
    - name: Upload data artifacts
      uses: actions/upload-artifact@v4
      with:
        name: hourly-data-pipeline-artifacts
        path: |
          data_repositories/
        retention-days: 7
        
    - name: Pipeline summary
      run: |
        echo ""
        echo "ğŸ‰ HOURLY DATA PIPELINE COMPLETED SUCCESSFULLY!"
        echo "=================================================================="
        echo "ğŸ“Š Pipeline executed at: $(date -u)"
        echo "ğŸ”„ Data collection: 6 hours of new data collected"
        echo "ğŸ“ Data merged into: data_repositories/historical_data/processed/merged_data.csv"
        echo "ğŸ”§ Script 02 executed: Data preprocessing completed"
        echo "ğŸ¯ Script 03 executed: Feature selection completed"
        echo "ğŸ’¾ All datasets committed to GitHub repository"
        echo "ğŸ“‚ Artifacts uploaded for next pipeline steps"
        echo "ğŸ”„ Next run scheduled for: $(date -u -d '+1 hour')"
        echo "=================================================================="
        
        # Final verification
        echo ""
        echo "ğŸ” FINAL VERIFICATION:"
        if [ -f "data_repositories/historical_data/processed/merged_data.csv" ]; then
          echo "âœ… Historical merged dataset: READY"
        else
          echo "âŒ Historical merged dataset: MISSING"
        fi
        
        if [ -f "data_repositories/features/phase1_no_leakage_data.csv" ]; then
          echo "âœ… Preprocessed features: READY"
        else
          echo "âŒ Preprocessed features: MISSING"
        fi
        
        if [ -f "data_repositories/features/phase1_fixed_selected_features.csv" ]; then
          echo "âœ… Selected features: READY"
        else
          echo "âŒ Selected features: MISSING"
        fi
        
        echo ""
        echo "ğŸš€ Pipeline ready for next steps (ML training, forecasting, etc.)"
