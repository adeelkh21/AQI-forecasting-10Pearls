name: Hourly Data Pipeline (Every Hour - 6 Hours Collection)

on:
  schedule:
    # Run every hour
    - cron: '0 * * * *'
  workflow_dispatch: # Allow manual trigger

jobs:
  data-pipeline:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements-ci.txt
        echo "‚úÖ Dependencies installed successfully"
        
    - name: Create data directories
      run: |
        mkdir -p data_repositories/hourly_data/raw
        mkdir -p data_repositories/hourly_data/processed
        mkdir -p data_repositories/historical_data/raw
        mkdir -p data_repositories/historical_data/processed
        mkdir -p data_repositories/features
        
    - name: Set up environment variables
      run: |
        echo "OPENWEATHER_API_KEY=${{ secrets.OPENWEATHER_API_KEY }}" >> $GITHUB_ENV
        echo "METEOSTAT_API_KEY=${{ secrets.METEOSTAT_API_KEY }}" >> $GITHUB_ENV
        
    - name: Load and verify historical data
      run: |
        echo "üìÇ Loading historical data from repository..."
        if [ -f "data_repositories/historical_data/processed/merged_data.csv" ]; then
          echo "‚úÖ Historical data found: data_repositories/historical_data/processed/merged_data.csv"
          echo "üìä Historical data info:"
          python -c "import pandas as pd; df = pd.read_csv('data_repositories/historical_data/processed/merged_data.csv'); print(f'Records: {len(df):,}'); print(f'Columns: {len(df.columns)}'); print(f'Date range: {df[\"timestamp\"].min()} to {df[\"timestamp\"].max()}' if 'timestamp' in df.columns else 'No timestamp column')"
        else
          echo "‚ö†Ô∏è No historical data found. Will create new dataset."
        fi
        
    - name: Collect new data (smart collection)
      run: |
        echo "üîÑ Starting smart data collection..."
        echo "üìÖ Current time: $(date -u)"
        echo "‚è∞ Automatically detecting last timestamp and collecting new data..."
        
        # Run the smart data collection script
        python collect_6hours.py
        
        echo "‚úÖ Data collection completed"
    
    - name: Finalize merged dataset (resolve pending file)
      run: |
        set -euo pipefail
        MASTER="data_repositories/historical_data/processed/merged_data.csv"
        # Find newest pending file (if any)
        LATEST_PENDING=$(find data_repositories/historical_data/processed -maxdepth 1 -type f -name 'merged_data.csv.pending_*' -printf '%T@ %p\n' | sort -nr | awk 'NR==1{print $2}')
        if [ ! -f "$MASTER" ] && [ -n "${LATEST_PENDING:-}" ]; then
          echo "‚ö†Ô∏è Merged dataset missing; promoting pending file: $LATEST_PENDING"
          mv "$LATEST_PENDING" "$MASTER"
        else
          echo "‚ÑπÔ∏è No promotion needed (master present or no pending file)"
        fi
        
    - name: Verify merged dataset
      run: |
        set -euo pipefail
        echo "üîç Verifying merged dataset after collection..."
        MASTER="data_repositories/historical_data/processed/merged_data.csv"
        if [ -f "$MASTER" ]; then
          echo "‚úÖ Merged dataset updated successfully"
          echo "üìä Updated dataset info:"
          python << 'PY'
          import pandas as pd
          try:
              df = pd.read_csv('data_repositories/historical_data/processed/merged_data.csv')
              print(f'   Total records: {len(df):,}')
              if 'timestamp' in df.columns:
                  df['timestamp'] = pd.to_datetime(df['timestamp'])
                  print(f'   Date range: {df["timestamp"].min()} to {df["timestamp"].max()}')
                  print(f'   Latest timestamp: {df["timestamp"].max()}')
              print(f'   Total columns: {len(df.columns)}')
          except Exception as e:
              print(f'   Error reading updated dataset: {e}')
          PY
        else
          echo "‚ùå Merged dataset not found after collection ‚Äì checking for pending file..."
          LATEST_PENDING=$(find data_repositories/historical_data/processed -maxdepth 1 -type f -name 'merged_data.csv.pending_*' -printf '%T@ %p\n' | sort -nr | awk 'NR==1{print $2}')
          if [ -n "${LATEST_PENDING:-}" ]; then
            echo "üîÑ Found pending file: $LATEST_PENDING"
            mv "$LATEST_PENDING" "$MASTER"
            echo "‚úÖ Promoted pending file to merged_data.csv"
            python << 'PY'
            import pandas as pd
            try:
                df = pd.read_csv('data_repositories/historical_data/processed/merged_data.csv')
                print(f'   Total records: {len(df):,}')
                if 'timestamp' in df.columns:
                    df['timestamp'] = pd.to_datetime(df['timestamp'])
                    print(f'   Date range: {df["timestamp"].min()} to {df["timestamp"].max()}')
                    print(f'   Latest timestamp: {df["timestamp"].max()}')
                print(f'   Total columns: {len(df.columns)}')
            except Exception as e:
                print(f'   Error reading updated dataset: {e}')
            PY
          else
            echo "‚ùå No pending file found either; failing the pipeline"
            exit 1
          fi
        fi
        
    - name: Run data preprocessing (script 02)
      run: |
        echo "üîÑ Starting data preprocessing (script 02)..."
        echo "üìÅ Input: data_repositories/historical_data/processed/merged_data.csv"
        python phase2_data_preprocessing.py
        
        # Verify output
        if [ -f "data_repositories/features/phase1_no_leakage_data.csv" ]; then
          echo "‚úÖ Preprocessing completed successfully"
          echo "üìä Preprocessing output info:"
          python -c "import pandas as pd; df = pd.read_csv('data_repositories/features/phase1_no_leakage_data.csv'); print(f'Records: {len(df):,}'); print(f'Features: {len(df.columns)}'); print(f'Date range: {df[\"timestamp\"].min()} to {df[\"timestamp\"].max()}' if 'timestamp' in df.columns else 'No timestamp column')"
        else
          echo "‚ùå Preprocessing output not found"
          exit 1
        fi
        
    - name: Run feature selection (script 03)
      run: |
        echo "üîÑ Starting feature selection (script 03)..."
        echo "üìÅ Input: data_repositories/features/phase1_no_leakage_data.csv"
        python phase3_feature_selection.py
        
        # Verify output
        if [ -f "data_repositories/features/phase1_fixed_selected_features.csv" ]; then
          echo "‚úÖ Feature selection completed successfully"
          echo "üìä Feature selection output info:"
          python -c "import pandas as pd; df = pd.read_csv('data_repositories/features/phase1_fixed_selected_features.csv'); print(f'Records: {len(df):,}'); print(f'Selected features: {len(df.columns)}'); print(f'Date range: {df[\"timestamp\"].min()} to {df[\"timestamp\"].max()}' if 'timestamp' in df.columns else 'No timestamp column')"
        else
          echo "‚ùå Feature selection output not found"
          exit 1
        fi
        
    - name: Commit and push all updated data
      run: |
        echo "üíæ Committing all updated datasets to GitHub..."
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        
        # Add all data files
        git add data_repositories/
        
        # Check if there are changes to commit
        if git diff --staged --quiet; then
          echo "‚ÑπÔ∏è No changes to commit"
        else
          # Get current timestamp for commit message
          TIMESTAMP=$(date -u +'%Y-%m-%d %H:%M:%S UTC')
          
          # Create detailed commit message
          git commit -m "ü§ñ Auto-update: Hourly data pipeline - $TIMESTAMP - 6 hours data collected, scripts 02 and 03 executed, all datasets committed"
          
          # Push to GitHub
          git push
          echo "‚úÖ All datasets committed and pushed to GitHub successfully"
        fi
        
    - name: Upload data artifacts
      uses: actions/upload-artifact@v4
      with:
        name: hourly-data-pipeline-artifacts
        path: |
          data_repositories/
        retention-days: 7
        
    - name: Pipeline summary
      run: |
        echo ""
        echo "üéâ HOURLY DATA PIPELINE COMPLETED SUCCESSFULLY!"
        echo "=================================================================="
        echo "üìä Pipeline executed at: $(date -u)"
        echo "üîÑ Data collection: Smart collection from last timestamp"
        echo "üìÅ Data merged into: data_repositories/historical_data/processed/merged_data.csv"
        echo "üîß Script 02 executed: Data preprocessing completed"
        echo "üéØ Script 03 executed: Feature selection completed"
        echo "üíæ All datasets committed to GitHub repository"
        echo "üìÇ Artifacts uploaded for next pipeline steps"
        echo "üîÑ Next run scheduled for: $(date -u -d '+1 hour')"
        echo "=================================================================="
        
        # Final verification
        echo ""
        echo "üîç FINAL VERIFICATION:"
        if [ -f "data_repositories/historical_data/processed/merged_data.csv" ]; then
          echo "‚úÖ Historical merged dataset: READY"
        else
          echo "‚ùå Historical merged dataset: MISSING"
        fi
        
        if [ -f "data_repositories/features/phase1_no_leakage_data.csv" ]; then
          echo "‚úÖ Preprocessed features: READY"
        else
          echo "‚ùå Preprocessed features: MISSING"
        fi
        
        if [ -f "data_repositories/features/phase1_fixed_selected_features.csv" ]; then
          echo "‚úÖ Selected features: READY"
        else
          echo "‚ùå Selected features: MISSING"
        fi
        
        echo ""
        echo "üöÄ Pipeline ready for next steps (ML training, forecasting, etc.)"
