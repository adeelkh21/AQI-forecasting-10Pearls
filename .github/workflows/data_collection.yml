name: Data Collection Pipeline

on:
  schedule:
    - cron: '0 * * * *'  # Run every hour
  workflow_dispatch:  # Allow manual trigger

# Prevent multiple runs from overlapping
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  collect_data:
    runs-on: ubuntu-latest
    timeout-minutes: 10  # Timeout after 10 minutes
    permissions:
      contents: read
      issues: write
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.10'
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Create data directories
      run: |
        mkdir -p data/raw
        mkdir -p data/processed
        mkdir -p data/logs
    
    - name: Run data collection
      env:
        OPENWEATHER_API_KEY: ${{ secrets.OPENWEATHER_API_KEY }}
      run: |
        python data_collection.py
      continue-on-error: true
    
    - name: Check for data files
      run: |
        if [ ! -d "data" ]; then
          echo "Data directory not found!"
          exit 1
        fi
        echo "Contents of data directory:"
        ls -R data/
    
    - name: Upload data artifacts
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: collected-data-${{ github.run_number }}-${{ github.run_attempt }}
        path: |
          data/
          *.log
        retention-days: 7
    
    - name: Check data quality
      if: success()
      run: |
        python - <<EOF
        import pandas as pd
        import sys
        import glob
        import os
        from data_validation import DataValidator

        def check_data_quality():
            # List all files in data directory for debugging
            print("Current directory contents:")
            for root, dirs, files in os.walk('.'):
                print(f"\nDirectory: {root}")
                for f in files:
                    print(f"  {f}")

            # Find the latest data file
            data_dirs = []
            for root, dirs, files in os.walk('data'):
                if 'processed' in dirs:
                    data_dirs.append(os.path.join(root, 'processed'))
            
            if not data_dirs:
                raise FileNotFoundError("No processed data directories found!")
                
            latest_dir = max(data_dirs, key=os.path.getctime)
            data_file = os.path.join(latest_dir, 'merged_data.csv')
            
            if not os.path.exists(data_file):
                raise FileNotFoundError(f"Data file not found: {data_file}")
            
            print(f"\nValidating data file: {data_file}")
            
            # Load and validate data
            df = pd.read_csv(data_file)
            
            # Convert timestamp to datetime
            df['timestamp'] = pd.to_datetime(df['timestamp'])
            
            print("\nData Preview:")
            print("=============")
            print("First few rows:")
            print(df.head())
            print("\nData Types:")
            print(df.dtypes)
            
            validator = DataValidator()
            is_valid, report = validator.validate_merged_data(df)
            
            print("\nValidation Report:")
            print("==================")
            print(f"Records: {len(df)}")
            print(f"Columns: {', '.join(df.columns)}")
            print(f"Date Range: {df['timestamp'].min()} to {df['timestamp'].max()}")
            print(f"Validation Status: {'✅ Passed' if is_valid else '❌ Failed'}")
            print("\nDetailed Report:")
            print(report)
            
            if not is_valid:
                raise ValueError("Data validation failed!")

        try:
            check_data_quality()
        except Exception as e:
            print(f"\n❌ Error during validation: {str(e)}")
            sys.exit(1)
        EOF
    
    - name: Notify on failure
      if: failure()
      uses: actions/github-script@v6
      with:
        script: |
          const issue = await github.rest.issues.create({
            owner: context.repo.owner,
            repo: context.repo.repo,
            title: 'Data Collection Pipeline Failed',
            body: `Pipeline failed on ${new Date().toISOString()}\n\nCheck the [workflow run](${context.serverUrl}/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}) for details.`
          });