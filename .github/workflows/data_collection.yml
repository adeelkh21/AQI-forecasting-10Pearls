name: Data Collection Pipeline

# Updated workflow with enhanced data collection and merging capabilities
# Version: 2.0 - Fixed file references and added comprehensive data processing

on:
  schedule:
    - cron: '0 * * * *'  # Run every hour
  workflow_dispatch:  # Allow manual trigger

# Prevent multiple runs from overlapping
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  collect_data:
    runs-on: ubuntu-latest
    timeout-minutes: 15  # Increased timeout for data processing
    permissions:
      contents: read
      issues: write
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.10'
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Create data repository directories
      run: |
        mkdir -p data_repositories/hourly_data/{raw,processed,metadata}
        mkdir -p data_repositories/merged_data/{raw,processed,metadata}
        mkdir -p data_repositories/historical_data/{raw,processed,metadata}
    
    - name: Check existing historical data
      run: |
        echo "üîç Checking existing historical data..."
        if [ -f "data_repositories/historical_data/processed/historical_merged.csv" ]; then
          echo "‚úÖ Found existing historical data"
          wc -l data_repositories/historical_data/processed/historical_merged.csv
          ls -la data_repositories/historical_data/processed/
        else
          echo "‚ö†Ô∏è No existing historical data found - will create new dataset"
        fi
    
    - name: Run hourly data collection
      env:
        OPENWEATHER_API_KEY: ${{ secrets.OPENWEATHER_API_KEY }}
      run: |
        echo "üïê Starting hourly data collection..."
        python phase1_data_collection.py
        
        # Check if new data was collected
        if [ -d "data_repositories/hourly_data/processed" ]; then
          echo "üìä New hourly data collected:"
          ls -la data_repositories/hourly_data/processed/
        else
          echo "‚ö†Ô∏è No new hourly data found"
        fi
        
        echo "‚úÖ Data collection completed successfully"
    
    - name: Merge with historical data
      run: |
        echo "üîÑ Merging new data with historical dataset..."
        
        python - <<EOF
        import pandas as pd
        import os
        from datetime import datetime
        import json
        import sys
        
        try:
            # Paths
            historical_path = "data_repositories/historical_data/processed/historical_merged.csv"
            hourly_data_dir = "data_repositories/hourly_data/processed"
            
            # Load existing historical data
            if os.path.exists(historical_path):
                print(f"üìñ Loading existing historical data: {historical_path}")
                historical_df = pd.read_csv(historical_path)
                print(f"   Records: {len(historical_df):,}")
                print(f"   Columns: {', '.join(historical_df.columns)}")
                
                # Check timestamp column
                if 'timestamp' in historical_df.columns:
                    historical_df['timestamp'] = pd.to_datetime(historical_df['timestamp'])
                    print(f"   Date Range: {historical_df['timestamp'].min()} to {historical_df['timestamp'].max()}")
            else:
                print("üìù No existing historical data - creating new dataset")
                historical_df = pd.DataFrame()
            
            # Find new hourly data files
            new_data_files = []
            if os.path.exists(hourly_data_dir):
                for file in os.listdir(hourly_data_dir):
                    if file.endswith('.csv'):
                        new_data_files.append(os.path.join(hourly_data_dir, file))
            
            if not new_data_files:
                print("‚ö†Ô∏è No new hourly data files found")
                if len(historical_df) > 0:
                    print("üíæ Keeping existing historical data unchanged")
                    sys.exit(0)
                else:
                    print("‚ùå No data available - cannot proceed")
                    sys.exit(1)
            
            # Load and merge new data
            merged_df = historical_df.copy()
            
            for file_path in new_data_files:
                print(f"üì• Loading new data: {os.path.basename(file_path)}")
                try:
                    new_df = pd.read_csv(file_path)
                    print(f"   New records: {len(new_df):,}")
                    
                    # Ensure timestamp column exists
                    if 'timestamp' in new_df.columns:
                        new_df['timestamp'] = pd.to_datetime(new_df['timestamp'])
                        
                        # Remove duplicates based on timestamp
                        if len(merged_df) > 0:
                            # Convert timestamp to string for comparison
                            merged_df['timestamp_str'] = merged_df['timestamp'].astype(str)
                            new_df['timestamp_str'] = new_df['timestamp'].astype(str)
                            
                            # Find new records (not in existing data)
                            existing_timestamps = set(merged_df['timestamp_str'])
                            new_records = new_df[~new_df['timestamp_str'].isin(existing_timestamps)]
                            
                            print(f"   New unique records: {len(new_records):,}")
                            
                            # Remove temporary column and append
                            new_records = new_records.drop('timestamp_str', axis=1)
                            merged_df = merged_df.drop('timestamp_str', axis=1)
                            
                            if len(new_records) > 0:
                                merged_df = pd.concat([merged_df, new_records], ignore_index=True)
                                print(f"   ‚úÖ Added {len(new_records)} new records")
                            else:
                                print(f"   ‚ö†Ô∏è No new unique records to add")
                        else:
                            # First time - add all data
                            merged_df = new_df.copy()
                            print(f"   ‚úÖ Added {len(new_df)} records (first dataset)")
                    else:
                        print(f"   ‚ö†Ô∏è No timestamp column found in {file_path}")
                        
                except Exception as e:
                    print(f"   ‚ùå Error loading {file_path}: {e}")
                    continue
            
            # Sort by timestamp
            if len(merged_df) > 0 and 'timestamp' in merged_df.columns:
                merged_df = merged_df.sort_values('timestamp').reset_index(drop=True)
                print(f"\nüìä Final merged dataset:")
                print(f"   Total Records: {len(merged_df):,}")
                print(f"   Date Range: {merged_df['timestamp'].min()} to {merged_df['timestamp'].max()}")
                print(f"   Columns: {', '.join(merged_df.columns)}")
                
                # Save merged dataset
                output_path = "data_repositories/historical_data/processed/historical_merged.csv"
                merged_df.to_csv(output_path, index=False)
                print(f"üíæ Saved merged dataset to: {output_path}")
                
                # Create metadata
                metadata = {
                    "last_updated": datetime.now().isoformat(),
                    "total_records": len(merged_df),
                    "date_range": {
                        "start": merged_df['timestamp'].min().isoformat(),
                        "end": merged_df['timestamp'].max().isoformat()
                    },
                    "columns": list(merged_df.columns),
                    "data_sources": ["historical_data", "hourly_collection"],
                    "workflow_run": os.environ.get('GITHUB_RUN_NUMBER', 'unknown')
                }
                
                metadata_path = "data_repositories/historical_data/metadata/dataset_info.json"
                os.makedirs(os.path.dirname(metadata_path), exist_ok=True)
                
                with open(metadata_path, 'w') as f:
                    json.dump(metadata, f, indent=2)
                print(f"üìã Saved metadata to: {metadata_path}")
                
            else:
                print("‚ùå No valid data to save")
                sys.exit(1)
                
        except Exception as e:
            print(f"‚ùå Error during data merge: {e}")
            import traceback
            traceback.print_exc()
            sys.exit(1)
        EOF
    
    - name: Validate merged dataset
      run: |
        echo "üîç Validating merged dataset..."
        
        python - <<EOF
        import pandas as pd
        import sys
        
        try:
            # Load the merged dataset
            df = pd.read_csv('data_repositories/historical_data/processed/historical_merged.csv')
            
            print("üìä Dataset Validation Results:")
            print(f"‚úÖ Total Records: {len(df):,}")
            print(f"‚úÖ Columns: {len(df.columns)}")
            print(f"‚úÖ Column Names: {', '.join(df.columns)}")
            
            # Check for required columns
            required_columns = ['timestamp']
            missing_columns = [col for col in required_columns if col not in df.columns]
            
            if missing_columns:
                print(f"‚ùå Missing required columns: {missing_columns}")
                sys.exit(1)
            else:
                print("‚úÖ All required columns present")
            
            # Validate timestamp column
            if 'timestamp' in df.columns:
                df['timestamp'] = pd.to_datetime(df['timestamp'])
                print(f"‚úÖ Timestamp Range: {df['timestamp'].min()} to {df['timestamp'].max()}")
                
                # Check for duplicates
                duplicates = df.duplicated(subset=['timestamp']).sum()
                if duplicates > 0:
                    print(f"‚ö†Ô∏è Found {duplicates} duplicate timestamps")
                else:
                    print("‚úÖ No duplicate timestamps")
            
            # Check for missing values
            missing_values = df.isnull().sum()
            total_missing = missing_values.sum()
            
            if total_missing > 0:
                print(f"‚ö†Ô∏è Total missing values: {total_missing}")
                print("Missing values by column:")
                for col, missing in missing_values.items():
                    if missing > 0:
                        print(f"  - {col}: {missing}")
            else:
                print("‚úÖ No missing values found")
            
            # Data type information
            print("\nüìã Data Types:")
            for col, dtype in df.dtypes.items():
                print(f"  - {col}: {dtype}")
            
            print("\n‚úÖ Dataset validation completed successfully!")
            
        except Exception as e:
            print(f"‚ùå Validation failed: {e}")
            sys.exit(1)
        EOF
    
    - name: Check final data files
      run: |
        echo "üìÅ Final data repository contents:"
        ls -R data_repositories/
        
        echo "\nüìä Historical data file size:"
        if [ -f "data_repositories/historical_data/processed/historical_merged.csv" ]; then
          wc -l data_repositories/historical_data/processed/historical_merged.csv
          du -h data_repositories/historical_data/processed/historical_merged.csv
        fi
    
    - name: Upload data artifacts
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: aqi-data-${{ github.run_number }}-${{ github.run_attempt }}
        path: |
          data_repositories/hourly_data/
          data_repositories/merged_data/
          data_repositories/historical_data/
        retention-days: 7
    
    - name: Notify on failure
      if: failure()
      uses: actions/github-script@v6
      with:
        script: |
          const issue = await github.rest.issues.create({
            owner: context.repo.owner,
            repo: context.repo.repo,
            title: 'Data Collection Pipeline Failed',
            body: `Pipeline failed on ${new Date().toISOString()}\n\nCheck the [workflow run](${context.serverUrl}/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}) for details.`
          });